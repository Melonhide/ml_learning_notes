
# 🧠 XGBoost Project Overview

This note outlines a real-world-style XGBoost modeling project used for borrower delinquency prediction, including background, model details, hyperparameter tuning, feature selection, and common interview questions.

---

## 📘 Project Summary

> Developed a XGBoost-based classification model, trained on historical borrower repayment data and financial indicators to predict potential delinquencies, achieving a 15% reduction in false positive alerts through hyperparameter tuning and feature selection

---

## ✅ Project Explanation (STAR Format)

### ⭐ Situation

We were noticing a high number of false positive alerts in our early delinquency detection system. Loan officers had to manually investigate many “risky” borrowers who turned out to be safe, wasting time and slowing down operations.

### 🎯 Task

I was tasked with building a more accurate risk prediction model that could reduce false alarms while maintaining high recall for real delinquents.

### 🔧 Action

I developed an XGBoost-based classifier trained on historical borrower repayment behavior and financial indicators such as:

- Debt-to-Income ratio
- Loan-to-Value ratio
- Credit score history
- Recent missed payments
- Macroeconomic context (Interest Rates)

After benchmarking Logistic Regression and Random Forest, XGBoost was chosen for its:
- Ability to capture non-linear interactions
- Built-in handling of missing values
- Feature importance capabilities

I tuned hyperparameters such as `max_depth`, `eta`, `min_child_weight`, `subsample`, and used `early_stopping_rounds`. Feature selection was done via Recursive Feature Elimination (RFE), and model interpretability was enhanced with SHAP.

### ✅ Result

Compared to our previous scoring method, the new XGBoost model reduced false positives by 15%, while maintaining recall above 85%. Loan reviewers reported significantly less time wasted on “safe” accounts.

---

## ❓ Common Interview Questions + Answers

### 1. Why use XGBoost over Logistic Regression or Random Forest?

I initially tested several models including logistic regression and random forest. Logistic regression was too limited in capturing non-linear patterns in our feature space. Random forest performed reasonably but lacked the fine control and regularization that XGBoost offers.
XGBoost gave better precision and recall, especially after tuning, and its native handling of missing values was a big advantage for our dataset.

Logistic Regression assumes a linear relationship between the input features and the log-odds of the positive class. It is simple, fast, and interpretable, but struggles when there are complex non-linear patterns or interactions among features. In our case, the borrower risk features had non-linear thresholds (e.g., sudden risk increase when LTV > 0.8), which made tree-based models like XGBoost more suitable.

Unlike XGBoost, Random Forest lacks explicit regularization and does not optimize trees sequentially. Its trees are built independently using bootstrapped data, which reduces variance but lacks the feedback loop and error correction found in boosting frameworks.

---

### 2. What features did you use and how did you select them?

We started with around 25 features derived from borrower demographics, loan characteristics, and repayment history.
After initial testing, I applied Recursive Feature Elimination with 5-fold CV to filter redundant or noisy features.
I then used SHAP to explain and validate the final set — features like LTV ratio, past delinquencies, and debt-to-income emerged as top contributors, which aligned well with domain expectations.

---

### 3. How did you tune hyperparameters?

I tuned key hyperparameters using a mix of grid search and early stopping.
Parameters like max_depth, eta, and min_child_weight had the biggest impact.
By lowering max_depth and increasing min_child_weight, we effectively reduced overfitting.
After tuning, we achieved a 15% drop in false positive rate without sacrificing recall.

---

### 4. How did you avoid overfitting?

We monitored both training and validation performance at each boosting round.
I implemented early stopping with a patience of 20 rounds.
In addition, we did 5-fold cross-validation and finally tested the model on a holdout set.
The validation AUC and recall remained stable, and real-world business impact (reduced unnecessary investigations) confirmed its generalization ability.

---

### 5. How did you explain model output?

We used SHAP to explain both global feature importance and individual predictions.
For example, when a borrower was flagged as high-risk, SHAP could show that a high LTV ratio and recent delinquencies contributed most to the score.
This allowed loan reviewers to quickly understand and trust the model output, and made it easier to align with compliance.

---

### 6. What challenges did you face?

One key challenge was class imbalance — only ~7% of borrowers actually defaulted.
I addressed this by adjusting the classification threshold and using class weights.
Another challenge was model explainability — to gain business adoption, I used SHAP to visualize decision paths and presented findings to risk analysts. This helped build trust and drove adoption.

---

## 📌 Why XGBoost – Detailed

### ✅ Handles Missing Values Automatically

XGBoost learns a default direction (left/right) for missing values at each split by comparing loss. No need for imputation.

---

### ✅ Built-in Regularization

Adds penalty to trees via:

$$
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2
$$

- $T$: number of leaves  
- $w_j$: leaf output  
- $\gamma$: complexity penalty  
- $\lambda$: weight penalty

Controls model size and prevents overfitting.

---

### ✅ Second-Order Optimization

Uses both gradient and hessian of loss function, like Newton’s method, for more stable and faster convergence.

---

### ✅ Parallel Training

Uses column block structure and multi-threaded split finding for faster training on large datasets.

---

## 📈 Summary

XGBoost was ideal for tabular, structured financial data. With strong regularization, built-in support for missing values, and great performance on imbalanced datasets, it improved business outcomes significantly while maintaining interpretability.
